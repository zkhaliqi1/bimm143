---
title: "class09"
author: "Zoheb Khaliqi"
date: "2/4/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# K-means clustering
The main k-means function in R is called 'kmeans()'. Let's play with it here.

```{r}
#Generate some example data for clustering
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))

plot(x)
```

Use kmeans() function setting k to 2 and nstart=20

```{r}
km <- kmeans(x, centers = 2, nstart = 20)
km
```
```{r}
km$size # Gives cluster size
```
```{r}
km$cluster # Gives cluster assignment/membership
```
```{r}
km$centers # Gives cluster center
```

```{r}
# Plot x colored by the kmeans cluster assignment and add cluster centers as blue points
plot(x, col=km$cluster+1)
points(km$centers, col="blue", pch=15)
```

# Hierarchical clustering in R

The main Hierarchical clustering function in R is called 'hclust()'
An important point here is that you have to calculate the distance matrix deom your impot data before calling 'hclust()'.

For this we will use the 'dist()'  function first.

```{r}
# We will use our x again from above
d <- dist(x)
hc <- hclust(d)
hc
```

Folks often view the results of Hierarchical clustering grahicaly. 
Lets trypassing this to the 'plot' function.

```{r}
plot(hc)
abline(h=6, col="red", lty=2) # Draws a line at height 6; see the 2 clusters
abline(h=3, col="blue")
```

To get cluster membershup vector I need to "cut" tge tree at a certain height to yield my seperate cluster branches.

```{r}
cutree(hc, h=6)
```

```{r}
gp3 <- cutree(hc, k=6)
table(gp3)
```


```{r}
# Step 1. Generate some example data for clustering
x <- rbind(
 matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1
 matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
 matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
 rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
# Step 2. Plot the data without clustering
plot(x)
# Step 3. Generate colors for known clusters
# (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)
```


```{r}
hc2 <- hclust(dist(x))
plot(hc2)
abline(h=2, col="red")
abline(h=2.5, col="blue")
```

To get cluster membership vector use `cutree()`
```{r}
gp2 <- cutree(hc2, k=2)
table(gp2)
```

```{r}
gp4 <- cutree(hc2, k=3)
table(gp4)
```

Make a pplot with our cluster results
```{r}
plot(x, col=gp4)
```


In-Class Lab

```{r}
x <- read.csv("UK_foods.csv", row.names = 1)
x
```

Let's make some plots to explore our data a bit more

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

A "pairs" plot can help when we have small datasets like this one but often we are dealing with data that is too large for these approaches

```{r}
pairs(x, col=rainbow(10), pch=16)
```

Principal Component Analysis (PCA) with the `prcomp()` function

```{r}
pca <- prcomp(t(x))
#pca
```

What is in my result object `pca`? I can check the attributes...

```{r}
attributes(pca) 
```


```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2")
text(pca$x[,1], pca$x[,2], colnames(x), col=c("black", "red", "blue", "darkgreen"))
```

```{r}
summary(pca)
```







